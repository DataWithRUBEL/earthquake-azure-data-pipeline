

# Required each time the cluster is restarted which should be only on the first notebook as they run in order

tiers = ["bronze", "silver", "gold"]
adls_paths = {tier: f"abfss://{tier}@rtyjkjhjytre456y.dfs.core.windows.net/" for tier in tiers}

# Accessing paths
bronze_adls = adls_paths["bronze"]
silver_adls = adls_paths["silver"]
gold_adls = adls_paths["gold"] 

dbutils.fs.ls(bronze_adls)
dbutils.fs.ls(silver_adls)
dbutils.fs.ls(gold_adls)
     
[]

import requests
import json
from datetime import date, timedelta
     

start_date = date.today() - timedelta(1)
end_date = date.today()
     

# Construct the API URL with start and end dates provided by Data Factory, formatted for geojson output.
url = f"https://earthquake.usgs.gov/fdsnws/event/1/query?format=geojson&starttime={start_date}&endtime={end_date}"

try:
    # Make the GET request to fetch data
    response = requests.get(url)

    # Check if the request was successful
    response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)
    data = response.json().get('features', [])

    if not data:
        print("No data returned for the specified date range.")
    else:
        # Specify the ADLS path
        file_path = f"{bronze_adls}/{start_date}_earthquake_data.json"

        # Save the JSON data
        json_data = json.dumps(data, indent=4)
        dbutils.fs.put(file_path, json_data, overwrite=True)
        print(f"Data successfully saved to {file_path}")
except requests.exceptions.RequestException as e:
    print(f"Error fetching data from API: {e}")

     
Wrote 286876 bytes.
Data successfully saved to abfss://bronze@rtyjkjhjytre456y.dfs.core.windows.net//2025-01-07_earthquake_data.json

data[0]
     
{'type': 'Feature',
 'properties': {'mag': 4.5,
  'place': '19 km NNE of Metahāra, Ethiopia',
  'time': 1736292406702,
  'updated': 1736294300040,
  'tz': None,
  'url': 'https://earthquake.usgs.gov/earthquakes/eventpage/us6000pihl',
  'detail': 'https://earthquake.usgs.gov/fdsnws/event/1/query?eventid=us6000pihl&format=geojson',
  'felt': None,
  'cdi': None,
  'mmi': None,
  'alert': None,
  'status': 'reviewed',
  'tsunami': 0,
  'sig': 312,
  'net': 'us',
  'code': '6000pihl',
  'ids': ',us6000pihl,',
  'sources': ',us,',
  'types': ',origin,phase-data,',
  'nst': 18,
  'dmin': 3.736,
  'rms': 0.55,
  'gap': 146,
  'magType': 'mb',
  'type': 'earthquake',
  'title': 'M 4.5 - 19 km NNE of Metahāra, Ethiopia'},
 'geometry': {'type': 'Point', 'coordinates': [39.9762, 9.0693, 9.256]},
 'id': 'us6000pihl'}

# Define your variables
output_data = {
    "start_date": start_date.isoformat(),
    "end_date": end_date.isoformat(),
    "bronze_adls": bronze_adls,
    "silver_adls": silver_adls,
    "gold_adls": gold_adls
}

# Return the dictionary directly
dbutils.jobs.taskValues.set(key = "bronze_output", value = output_data)
     
